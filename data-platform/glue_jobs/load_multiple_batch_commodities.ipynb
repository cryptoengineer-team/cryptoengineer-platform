{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%iam_role arn:aws:iam::212430227630:role/LabRole\n%region us-east-1\n%number_of_workers 2\n\n%idle_timeout 30\n%glue_version 4.0\n%worker_type G.1X",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "Current iam_role is arn:aws:iam::212430227630:role/LabRole\niam_role has been set to arn:aws:iam::212430227630:role/LabRole.\nPrevious region: us-east-1\nSetting new region to: us-east-1\nRegion is set to: us-east-1\nPrevious number of workers: 2\nSetting new number of workers to: 2\nCurrent idle_timeout is 30 minutes.\nidle_timeout has been set to 30 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%extra_py_files s3://cryptoengineer/gluejobs-py-modules/load.py, s3://cryptoengineer/gluejobs-py-modules/storage.py\n%additional_python_modules yfinance",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "Extra py files to be included:\ns3://cryptoengineer/gluejobs-py-modules/load.py\ns3://cryptoengineer/gluejobs-py-modules/storage.py\nAdditional python modules to be included:\nyfinance\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%load_ext autoreload\n%autoreload 2",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nimport boto3\n\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 2\nIdle Timeout: 30\nSession ID: 19eefaae-215f-4152-9a5e-98d264cdaab8\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\n--extra-py-files s3://cryptoengineer/gluejobs-py-modules/load.py,s3://cryptoengineer/gluejobs-py-modules/storage.py\n--additional-python-modules yfinance\nWaiting for session 19eefaae-215f-4152-9a5e-98d264cdaab8 to get into ready status...\nSession 19eefaae-215f-4152-9a5e-98d264cdaab8 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Batch Load - COMMODITIES\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "from datetime import datetime, timedelta, timezone\nimport pandas as pd\n\nimport load",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Set AWS Storage parameters\n",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "BUCKET_NAME = \"cryptoengineer\"\nPREFIX = \"datalake/bronze/commodities\"",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Load job parameters",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "glue_client = boto3.client(\"glue\")\n\nif '--WORKFLOW_NAME' in sys.argv and '--WORKFLOW_RUN_ID' in sys.argv:\n    print(\"Running in Glue Workflow\")\n    \n    glue_args = getResolvedOptions(\n        sys.argv, ['WORKFLOW_NAME', 'WORKFLOW_RUN_ID']\n    )\n    \n    print(\"Reading the workflow parameters\")\n    workflow_args = glue_client.get_workflow_run_properties(\n        Name=glue_args['WORKFLOW_NAME'], RunId=glue_args['WORKFLOW_RUN_ID']\n    )[\"RunProperties\"]\n\n    \n    base= workflow_args['base']\n    time_frame = int(workflow_args['time_frame'])\n    freq= workflow_args['freq']\n    symbols = workflow_args['symbols']\n    api_key = workflow_args['api_key']\n\nelse:\n    try:\n        args = getResolvedOptions(sys.argv,\n                                  ['JOB_NAME',\n                                   'base',\n                                   'time_frame',\n                                   'freq',\n                                   'symbols',\n                                   'api_key'])\n        base= args['base']\n        time_frame = int(args['time_frame'])\n        freq = args['freq']\n        ##symbols = \"EUR,CHF,JPY,CAD,GBP\"\n        symbols = args['symbols']\n        api_key = args['api_key']\n        print(\"Running as Job\")        \n    except:\n        print(\"Running as Notebook\")\n        base = \"USD\"\n        time_frame = 43800 #48\n        freq= '1day' #'15min'\n        symbols = \"GCUSD\"\n        api_key = \"\"",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 41,
			"outputs": [
				{
					"name": "stdout",
					"text": "Running as Notebook\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(\"base: \", base)\nprint(\"Time Frame: \", time_frame)\nprint(\"Frequency: \", freq)\nprint(\"Symbols: \", symbols)\nprint(\"API Key: \", api_key)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Set the start and end dates for the data you want to load",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "# Start date\nstart_date = (datetime.utcnow() - timedelta(hours=time_frame)).strftime(\"%Y-%m-%d\")\nend_date = datetime.utcnow().strftime(\"%Y-%m-%d\")\n\nprint(\"Start date; \",start_date,\" End date: \",end_date)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 43,
			"outputs": [
				{
					"name": "stdout",
					"text": "Start date;  2019-09-02  End date:  2024-08-31\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Load the historical commodities- 15min frequency",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "Set some config values",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "type='COMMODITIES'\nsource='FMP'",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 44,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "if freq == '15min':\n    print(\"Reading historical data by frequency\")\n    df= pd.DataFrame()\n    for symbol in symbols.split(\",\"):\n        print(\"Loading: \", symbol)\n        symbol_df = load.load_batch_freq_rates(base=base,\n                                              start_date=start_date,\n                                              end_date=end_date,\n                                              freq=freq,\n                                              symbol=symbol,\n                                              api_key=api_key,\n                                              source=source\n        )\n        # Complete the table schema \n        if len(symbol_df)>0:\n            symbol_df = load.set_schema_table(symbol_df, symbol, source, freq, base, type)\n            print(\"Records: \", len(symbol_df))\n            df = pd.concat([df, symbol_df])\n        else:\n            print(\"No data for: \", symbol)\n\nelse:\n    print(\"Reading daily historical data\")\n    df= pd.DataFrame()\n    for symbol in symbols.split(\",\"):\n        print(\"Loading: \", symbol)\n        symbol_df = load.load_historical_rates(base=base,\n                                              start_date=start_date,\n                                              end_date=end_date,\n                                              symbol=symbol,\n                                              api_key=api_key,\n                                              source=source\n        )\n        # Complete the table schema \n        if len(symbol_df)>0:        \n            symbol_df = load.set_schema_table(symbol_df, symbol, source, freq, base, type)\n            print(\"Records: \", len(symbol_df))\n            df = pd.concat([df, symbol_df])\n        else:\n            print(\"No data for: \", symbol)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 45,
			"outputs": [
				{
					"name": "stdout",
					"text": "Reading daily historical data\nLoading:  GCUSD\nhttps://financialmodelingprep.com/api/v3/historical-price-full\nhttps://financialmodelingprep.com/api/v3/historical-price-full/GCUSD?apikey=xGFdE9Ydrcr1oCDJiCjHiZnkqUnQnjaH\nLectura API correcta\nRecords:  1302\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(\"Records: \", len(df))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 46,
			"outputs": [
				{
					"name": "stdout",
					"text": "Records:  1302\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.info()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 47,
			"outputs": [
				{
					"name": "stdout",
					"text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1302 entries, 0 to 1301\nData columns (total 25 columns):\n #   Column            Non-Null Count  Dtype              \n---  ------            --------------  -----              \n 0   datetime          1302 non-null   object             \n 1   open              1302 non-null   float64            \n 2   high              1302 non-null   float64            \n 3   low               1302 non-null   float64            \n 4   close             1302 non-null   float64            \n 5   adjClose          1302 non-null   float64            \n 6   volume            1302 non-null   int64              \n 7   unadjustedVolume  1302 non-null   int64              \n 8   change            1302 non-null   float64            \n 9   changePercent     1302 non-null   float64            \n 10  vwap              1302 non-null   float64            \n 11  label             1302 non-null   object             \n 12  changeOverTime    1302 non-null   float64            \n 13  year              1302 non-null   object             \n 14  month             1302 non-null   object             \n 15  day               1302 non-null   object             \n 16  time              1302 non-null   object             \n 17  date              1302 non-null   object             \n 18  base_currency     1302 non-null   object             \n 19  source            1302 non-null   object             \n 20  frequency         1302 non-null   object             \n 21  symbol            1302 non-null   object             \n 22  audit_time        1302 non-null   datetime64[ns, UTC]\n 23  load_date         1302 non-null   object             \n 24  type              1302 non-null   object             \ndtypes: datetime64[ns, UTC](1), float64(9), int64(2), object(13)\nmemory usage: 264.5+ KB\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.head(1)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 48,
			"outputs": [
				{
					"name": "stdout",
					"text": "     datetime    open  ...   load_date         type\n0  2024-08-30  2553.6  ...  2024-08-31  COMMODITIES\n\n[1 rows x 25 columns]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Append the batch data to RAW table",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "Set the destination raw table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "path=f\"s3://{BUCKET_NAME}/{PREFIX}\"\nprint(\"Path:\",path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 51,
			"outputs": [
				{
					"name": "stdout",
					"text": "Path: s3://cryptoengineer/datalake/raw/commodities\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "if len(df)>0:\n    print(\"Saving data to: \", path)    \n    (\n        spark.createDataFrame(df)\n        .repartition(\"load_date\")\n        .write\n        .format(\"parquet\")\n        .mode(\"append\")\n        .partitionBy(['load_date'])\n        .save(path)\n    )",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 52,
			"outputs": [
				{
					"name": "stdout",
					"text": "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}