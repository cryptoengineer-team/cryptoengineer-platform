{"metadata":{"kernelspec":{"name":"glue_pyspark","display_name":"Glue PySpark","language":"python"},"language_info":{"name":"Python_Glue_Session","mimetype":"text/x-python","codemirror_mode":{"name":"python","version":3},"pygments_lexer":"python3","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CARGA DE DATOS INFORMATIVOS EN LA CAPA SILVER\n##### Este notebook tiene como objetivo actualizar la información de la capa silver a partir de la máxima fecha de carga encontrada en bronze","metadata":{"editable":true,"trusted":true}},{"cell_type":"markdown","source":"####  Run this cell to set up and start your interactive session.\n","metadata":{"editable":true,"trusted":true}},{"cell_type":"code","source":"#%help\n%stop_session","metadata":{"trusted":true,"editable":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Stopping session: 44cd6e96-ed9a-4e6d-84e2-f176c3efdcdf\nStopped session.\n","output_type":"stream"}]},{"cell_type":"code","source":"%region us-east-1\n%number_of_workers 2\n%idle_timeout 30\n%worker_type G.1X\n%glue_version 4.0\n\nBUCKET = 'cryptoengineer'","metadata":{"trusted":true,"editable":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Previous region: us-east-1\nSetting new region to: us-east-1\nRegion is set to: us-east-1\nPrevious number of workers: 2\nSetting new number of workers to: 2\nCurrent idle_timeout is 30 minutes.\nidle_timeout has been set to 30 minutes.\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nSetting Glue version to: 4.0\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 2\nIdle Timeout: 30\nSession ID: 858ec864-ab7c-4527-b4ee-cbe9b8fd8819\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\nWaiting for session 858ec864-ab7c-4527-b4ee-cbe9b8fd8819 to get into ready status...\nSession 858ec864-ab7c-4527-b4ee-cbe9b8fd8819 has been created.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#Importación de librerías necesarias\nfrom pyspark.sql.types import StructField, StructType, StringType, DoubleType, IntegerType, TimestampType, DateType\nfrom pyspark.sql.functions import col, from_unixtime, lit, regexp_replace, current_date, min as spark_min, max as spark_max, upper, date_format\nimport boto3\nimport os\nimport sys\n\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)","metadata":{"trusted":true,"tags":[]},"execution_count":3,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"#Load job parameters\nglue_client = boto3.client(\"glue\")\n\nif '--WORKFLOW_NAME' in sys.argv and '--WORKFLOW_RUN_ID' in sys.argv:\n    print(\"Running in Glue Workflow\")\n    \n    glue_args = getResolvedOptions(\n        sys.argv, ['WORKFLOW_NAME', 'WORKFLOW_RUN_ID', 'type']\n    )\n    \n    print(glue_args)\n    \n    asset_type = glue_args['type']\n\nelse:\n    print(\"Running as Job\")\n    args = getResolvedOptions(sys.argv,\n                              ['JOB_NAME',\n                               'type'])\n\n    asset_type = args['type']","metadata":{"trusted":true,"tags":[]},"execution_count":3,"outputs":[{"name":"stdout","text":"GlueArgumentError: the following arguments are required: --JOB_NAME, --type\n","output_type":"stream"}]},{"cell_type":"code","source":"#asset_type = 'cryptos'","metadata":{"trusted":true,"tags":[]},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Carga las particiones disponibles\npartitions_df = spark.read.parquet('s3://' + BUCKET + f'/datalake/bronze/{asset_type}').select('load_date').distinct()\n\n# Encuentra el último load_date\nmax_load_date = partitions_df.agg(spark_max('load_date')).collect()[0][0]\n\nprint(f\"Último load_date encontrado: {max_load_date}\")","metadata":{"trusted":true,"tags":[]},"execution_count":5,"outputs":[{"name":"stdout","text":"Último load_date encontrado: 2024-08-23\n","output_type":"stream"}]},{"cell_type":"code","source":"#Definición del esquema\nschema = StructType([\n    StructField(\"SYMBOL\", StringType(), False),\n    StructField(\"BASE_CURRENCY\", StringType(), True),\n    StructField(\"TYPE\", StringType(), True),\n    StructField(\"DATETIME\", TimestampType(), True),\n    StructField(\"DATE\", DateType(), True),\n    StructField(\"TIME\", StringType(), True),\n    StructField(\"FREQUENCY\", StringType(), True),\n    StructField(\"YEAR\", IntegerType(), False),\n    StructField(\"MONTH\", IntegerType(), True),\n    StructField(\"DAY\", IntegerType(), True),\n    StructField(\"OPEN\", DoubleType(), True),\n    StructField(\"HIGH\", DoubleType(), True),\n    StructField(\"LOW\", DoubleType(), True),\n    StructField(\"CLOSE\", DoubleType(), True),\n    StructField(\"VOLUME\", DoubleType(), True),\n    StructField(\"TRADES\", IntegerType(), True),\n    StructField(\"AUDIT_TIME\", DateType(), True)\n])","metadata":{"trusted":true,"tags":[]},"execution_count":6,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Leer los datos correspondientes al último load_date\nlatest_data_df = (\n    spark.read\n    .schema(schema)\n    .parquet('s3://' + BUCKET + f'/datalake/bronze/{asset_type}')\n    .filter(col('load_date') == max_load_date)\n)\n\n# Verifica la estructura y muestra algunas filas\nlatest_data_df.printSchema()\nlatest_data_df.show(5)","metadata":{"trusted":true,"tags":[]},"execution_count":7,"outputs":[{"name":"stdout","text":"Py4JJavaError: An error occurred while calling o121.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 10) (172.39.254.209 executor 1): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file s3://cryptoengineer-lg/datalake/bronze/cryptos/load_date=2024-08-23/part-00008-d79ee3bc-10d5-487c-b9d1-2a26696122ba.c000.snappy.parquet. Column: [TRADES], Expected: int, Found: INT64\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:706)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:397)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:702)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:968)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:383)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1125)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:187)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:331)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:227)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:393)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2229)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2250)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:533)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:486)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3932)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:552)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3125)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:290)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:329)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file s3://cryptoengineer-lg/datalake/bronze/cryptos/load_date=2024-08-23/part-00008-d79ee3bc-10d5-487c-b9d1-2a26696122ba.c000.snappy.parquet. Column: [TRADES], Expected: int, Found: INT64\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:706)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:397)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:702)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:968)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:383)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1125)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:187)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:331)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:227)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:393)\n\t... 21 more\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#transformo datos para obtener la estructura de silver\ntransformed_data_df = (\n    latest_data_df\n    .withColumn('year', col('datetime').substr(0, 4))\n    .withColumn('month', col('datetime').substr(6, 2))\n    .withColumn('day', col('datetime').substr(9, 2))\n    .withColumn('frequency', lit('15min'))\n    .withColumn('base_currency', lit('USD'))\n    .withColumn(\"date\", date_format(col(\"datetime\"), \"yyyy-MM-dd\"))\n    .withColumn(\"time\", date_format(col(\"datetime\"), \"HH:mm:ss\"))\n    .select(\n        col('symbol').alias('SYMBOL'),\n        col('base_currency').alias('BASE_CURRENCY'),\n        lit(asset_type).alias('TYPE'),\n        col('datetime').alias('DATETIME'),\n        col('date').alias('DATE'),\n        col('time').alias('TIME'),\n        col('frequency').alias('FREQUENCY'),\n        col('year').cast(IntegerType()).alias('YEAR'),\n        col('month').cast(IntegerType()).alias('MONTH'),\n        col('day').cast(IntegerType()).alias('DAY'),\n        col('open').alias('OPEN'),\n        col('high').alias('HIGH'),\n        col('low').alias('LOW'),\n        col('close').alias('CLOSE'),\n        col('volume').alias('VOLUME'),\n        col('trades').alias('TRADES'),\n        current_date().alias('AUDIT_TIME')\n    )\n)\ntransformed_data_df.printSchema()\ntransformed_data_df.show(5)","metadata":{"trusted":true,"tags":[]},"execution_count":7,"outputs":[{"name":"stdout","text":"root\n |-- SYMBOL: string (nullable = true)\n |-- BASE_CURRENCY: string (nullable = false)\n |-- TYPE: string (nullable = false)\n |-- DATETIME: timestamp (nullable = true)\n |-- DATE: string (nullable = true)\n |-- TIME: string (nullable = true)\n |-- FREQUENCY: string (nullable = false)\n |-- YEAR: integer (nullable = true)\n |-- MONTH: integer (nullable = true)\n |-- DAY: integer (nullable = true)\n |-- OPEN: double (nullable = true)\n |-- HIGH: double (nullable = true)\n |-- LOW: double (nullable = true)\n |-- CLOSE: double (nullable = true)\n |-- VOLUME: double (nullable = true)\n |-- TRADE: integer (nullable = true)\n |-- AUDIT_TIME: date (nullable = false)\n\n+-------+-------------+-------+-------------------+----------+--------+---------+----+-----+---+-------+-------+-------+-------+----------+-----+----------+\n| SYMBOL|BASE_CURRENCY|   TYPE|           DATETIME|      DATE|    TIME|FREQUENCY|YEAR|MONTH|DAY|   OPEN|   HIGH|    LOW|  CLOSE|    VOLUME|TRADE|AUDIT_TIME|\n+-------+-------------+-------+-------------------+----------+--------+---------+----+-----+---+-------+-------+-------+-------+----------+-----+----------+\n|XBTUSDC|          USD|cryptos|2020-01-08 15:15:00|2020-01-08|15:15:00|    15min|2020|    1|  8| 8300.0| 8300.0| 8300.0| 8300.0|     0.002|    1|2024-08-23|\n|XBTUSDC|          USD|cryptos|2020-01-08 15:30:00|2020-01-08|15:30:00|    15min|2020|    1|  8| 8300.0| 8300.0| 8300.0| 8300.0| 4.0963E-4|    1|2024-08-23|\n|XBTUSDC|          USD|cryptos|2020-01-08 16:00:00|2020-01-08|16:00:00|    15min|2020|    1|  8|8217.51|8217.51|8217.51|8217.51|   0.00963|    1|2024-08-23|\n|XBTUSDC|          USD|cryptos|2020-01-08 16:15:00|2020-01-08|16:15:00|    15min|2020|    1|  8|8217.51|8217.51|8217.51|8217.51|   0.00266|    1|2024-08-23|\n|XBTUSDC|          USD|cryptos|2020-01-08 17:30:00|2020-01-08|17:30:00|    15min|2020|    1|  8|7900.01|7900.01|7900.01|7900.01|0.01415437|    3|2024-08-23|\n+-------+-------------+-------+-------------------+----------+--------+---------+----+-----+---+-------+-------+-------+-------+----------+-----+----------+\nonly showing top 5 rows\n","output_type":"stream"}]},{"cell_type":"code","source":"#persistimos los datos en la capa silver, reparticionando por simbolo y año, en modalidad append\n(\n    transformed_data_df\n    .write\n    .format('parquet')\n    .mode(\"append\")\n    .partitionBy(\"SYMBOL\", \"YEAR\")\n    .save('s3://' + BUCKET + f'/datalake/silver/{asset_type}')\n)\n\nprint('Datos guardados en s3://' + BUCKET +  f'/datalake/silver/{asset_type}')","metadata":{"trusted":true,"tags":[]},"execution_count":8,"outputs":[{"name":"stdout","text":"Datos guardados en s3://cryptoengineer-lg/datalake/silver/cryptos\n","output_type":"stream"}]}]}