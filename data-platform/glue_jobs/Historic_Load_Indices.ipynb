{"metadata":{"kernelspec":{"name":"glue_pyspark","display_name":"Glue PySpark","language":"python"},"language_info":{"name":"Python_Glue_Session","mimetype":"text/x-python","codemirror_mode":{"name":"python","version":3},"pygments_lexer":"python3","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CARGA HISTÓRICA DE DATOS\n##### El presente notebook tiene como objeto procesar los ficheros .csv obtenidos a partir del scraping de INVESTING\n","metadata":{"editable":true,"trusted":true}},{"cell_type":"markdown","source":"####  Run this cell to set up and start your interactive session.\n","metadata":{"editable":true,"trusted":true}},{"cell_type":"code","source":"#%help","metadata":{"trusted":true,"editable":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"%region us-east-1\n%number_of_workers 2\n%idle_timeout 30\n%worker_type G.1X\n%glue_version 4.0\n\nBUCKET = 'cryptoengineer'","metadata":{"trusted":true,"editable":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \nPrevious region: us-east-1\nSetting new region to: us-east-1\nRegion is set to: us-east-1\nPrevious number of workers: None\nSetting new number of workers to: 2\nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 30 minutes.\nPrevious worker type: None\nSetting new worker type to: G.1X\nSetting Glue version to: 4.0\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 2\nIdle Timeout: 30\nSession ID: 6c13aea1-a4d6-4d81-be91-3080bb95fe57\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\nWaiting for session 6c13aea1-a4d6-4d81-be91-3080bb95fe57 to get into ready status...\nSession 6c13aea1-a4d6-4d81-be91-3080bb95fe57 has been created.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#Importación de librerías necesarias\nfrom pyspark.sql.types import StructField, StructType, StringType, DoubleType, IntegerType, TimestampType, DateType\nfrom pyspark.sql.functions import col, from_unixtime, lit, regexp_replace, current_date, min as spark_min, max as spark_max, regexp_extract\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nimport boto3\nimport os\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)","metadata":{"trusted":true,"tags":[]},"execution_count":8,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"#Defino función para determinar los ficheros a procesar\ndef list_s3_files(bucket_name, folder_prefix):\n    s3 = boto3.client('s3')\n    paginator = s3.get_paginator('list_objects_v2')\n    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=folder_prefix)\n    \n    files = []\n    for page in page_iterator:\n        if 'Contents' in page:\n            for obj in page['Contents']:\n                # Check if the object is a file and not a directory\n                if not obj['Key'].endswith('/'):\n                    # Extract the file name from the full S3 key\n                    file_name = os.path.basename(obj['Key'])\n                    files.append(file_name)\n    return files","metadata":{"trusted":true,"tags":[]},"execution_count":3,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"#Lectura de ficheros CSV a procesar\nfiles = list_s3_files(BUCKET, 'datalake/historic_data/indices')\n\nprint('Los ficheros a procesar son: ')\nfor file in files:\n    print(file)","metadata":{"trusted":true,"tags":[]},"execution_count":4,"outputs":[{"name":"stdout","text":"Los ficheros a procesar son: \nDAX_20240904.csv\nDowJones_20240904.csv\nEuroStoxx50_20240904.csv\nIBEX35_20240904.csv\nNasdaq_20240904.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#Creación del esquema \nhistoric_schema = StructType([\n    StructField('TIMESTAMP', StringType(), True),\n    StructField('OPEN', DoubleType(), True),\n    StructField('HIGH', DoubleType(), True),\n    StructField('LOW', DoubleType(), True),\n    StructField('CLOSE', DoubleType(), True),\n    StructField('VOLUME', DoubleType(), True),\n    StructField('TRADES', IntegerType(), True),\n    StructField('ORIGIN', StringType(), True),\n    StructField('LOAD_DATE', DateType(), True),\n    StructField('SYMBOL', StringType(), True),\n    StructField('DATETIME', TimestampType(), True),\n    StructField('YEAR', IntegerType(), True)\n])\n\n#Creación del DF de destino\nhistoric_df = spark.createDataFrame([], historic_schema)","metadata":{"trusted":true,"tags":[]},"execution_count":5,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"#Iteración a través de todos los ficheros \nfor file in files:\n    print('Procesado el fichero: ' + file)\n    #Lectura del fichero\n    file_df = (\n        spark.read\n        .format(\"csv\")\n        .schema(historic_schema)\n        .option('header', 'false')\n        .load('s3://' + BUCKET + '/datalake/historic_data/indices/' + file)\n\n        #Transformaciones básicas\n        .withColumn('origin', lit('scraping'))\n        .withColumn('load_date', current_date())\n        .withColumn('symbol', regexp_extract(lit(file), r'^([^_]+)', 1))\n        .withColumn('datetime', from_unixtime(col('timestamp')).cast('timestamp'))\n        .withColumn('year', col('datetime').substr(0, 4).cast('int'))\n\n    )\n    historic_df = historic_df.unionAll(file_df)","metadata":{"trusted":true,"tags":[]},"execution_count":12,"outputs":[{"name":"stdout","text":"Procesado el fichero: DAX_20240904.csv\nProcesado el fichero: DowJones_20240904.csv\nProcesado el fichero: EuroStoxx50_20240904.csv\nProcesado el fichero: IBEX35_20240904.csv\nProcesado el fichero: Nasdaq_20240904.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"historic_df.printSchema()","metadata":{"trusted":true,"tags":[]},"execution_count":13,"outputs":[{"name":"stdout","text":"root\n |-- TIMESTAMP: string (nullable = true)\n |-- OPEN: double (nullable = true)\n |-- HIGH: double (nullable = true)\n |-- LOW: double (nullable = true)\n |-- CLOSE: double (nullable = true)\n |-- VOLUME: double (nullable = true)\n |-- TRADES: integer (nullable = true)\n |-- ORIGIN: string (nullable = true)\n |-- LOAD_DATE: date (nullable = true)\n |-- SYMBOL: string (nullable = true)\n |-- DATETIME: timestamp (nullable = true)\n |-- YEAR: integer (nullable = true)\n","output_type":"stream"}]},{"cell_type":"code","source":"historic_df.show(5)","metadata":{"trusted":true,"tags":[]},"execution_count":14,"outputs":[{"name":"stdout","text":"+----------+----------------+----------------+----------------+----------------+---------+------+--------+----------+------+-------------------+----+\n| TIMESTAMP|            OPEN|            HIGH|             LOW|           CLOSE|   VOLUME|TRADES|  ORIGIN| LOAD_DATE|SYMBOL|           DATETIME|YEAR|\n+----------+----------------+----------------+----------------+----------------+---------+------+--------+----------+------+-------------------+----+\n|1630911600| 15828.900390625| 15836.400390625| 15817.349609375| 15833.900390625|1615827.0|  null|scraping|2024-09-04|   DAX|2021-09-06 07:00:00|2021|\n|1630912500|15849.5498046875|15883.4501953125| 15845.900390625| 15874.900390625|1741831.0|  null|scraping|2024-09-04|   DAX|2021-09-06 07:15:00|2021|\n|1630913400|15878.4501953125|15886.4501953125| 15875.400390625|15880.9501953125| 882622.0|  null|scraping|2024-09-04|   DAX|2021-09-06 07:30:00|2021|\n|1630914300|15884.4501953125|15884.4501953125|15873.9501953125|15873.9501953125| 701598.0|  null|scraping|2024-09-04|   DAX|2021-09-06 07:45:00|2021|\n|1630915200|15875.4501953125|15879.4501953125| 15868.400390625| 15871.400390625| 612118.0|  null|scraping|2024-09-04|   DAX|2021-09-06 08:00:00|2021|\n+----------+----------------+----------------+----------------+----------------+---------+------+--------+----------+------+-------------------+----+\nonly showing top 5 rows\n","output_type":"stream"}]},{"cell_type":"code","source":"#Persistencia de datos\n(\n    historic_df\n        .write\n        .format('parquet')\n        #.partitionBy('symbol','year')\n        .partitionBy('LOAD_DATE')\n        .mode('append')\n        .save('s3://' + BUCKET + '/datalake/bronze/indices')\n)\n\nprint('Datos guardados en s3://' + BUCKET + '/datalake/bronze/indices')","metadata":{"trusted":true,"tags":[]},"execution_count":15,"outputs":[{"name":"stdout","text":"Datos guardados en s3://cryptoengineer-lg/datalake/bronze/indices\n","output_type":"stream"}]}]}